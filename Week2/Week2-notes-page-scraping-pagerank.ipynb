{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aea7d03",
   "metadata": {},
   "source": [
    "# Page scraping diagramming and calculating PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018fb89e",
   "metadata": {},
   "source": [
    "please make sure you have:\n",
    "* decorator >= 5.09 version\n",
    "* networkx is the latest\n",
    "* scipy == 1.8.0\n",
    "* requests is the latest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e3989d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade decorator\n",
    "!pip install --upgrade networkx\n",
    "!pip install scipy==1.8.0 # please make sure your scipy version is 1.8.0 in order to run scripy.sparse.coo_array in the package of network\n",
    "!pip install --upgrade requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff980359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx #please make sure you install decorator >= 5.09 version\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pprint as pp\n",
    "import requests\n",
    "import pathlib\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68a8abe",
   "metadata": {},
   "source": [
    "In this short notebool we will learn how to scrape links from a website. (Initially just a simple website where we load it as HTML with beautiful soup, with no need to Selenium chrome-pupeteering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf22b2a",
   "metadata": {},
   "source": [
    "- Then we will visit all the websites these links point to... and gather links from them. This way we will create a frontier and crawl/spider our way through the website and create its map.\n",
    "- Then we will represent that map as a simple graph.\n",
    "- And then we will scale the graph accordin to it's PageRank."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75458b0a",
   "metadata": {},
   "source": [
    "**But before we start: map all the links of a website by hand**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf837d8",
   "metadata": {},
   "source": [
    "Visit all pages pages in the demo folder and on paper create a maps of all the links. To view the same pages as a websit: run the code below (**Note**: you can just click the `html` file directly, but all the links probably do not work, `403` may return)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a367bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will make sure the local links will work, independly of your operating system\n",
    "def create_local_file_address(folder, file):\n",
    "    file_address =  os.path.join(os.getcwd(), folder, file)\n",
    "    with_schema = pathlib.Path(file_address).as_uri()\n",
    "    return with_schema\n",
    "\n",
    "\n",
    "# and this will give you the link to visit in your browser:\n",
    "print(\"copy this link into another tab in your browser and visit it:\\n\\n\")\n",
    "print(create_local_file_address(\"demowebsite\", \"home.html\"))\n",
    "\n",
    "# the link will look a bit like this: \n",
    "# file:///C:/WSpace/WebSNA-notes/Week2/demowebsite/home.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126dc44c",
   "metadata": {},
   "source": [
    "You need to spend a minute drawing all the links, only by clicking in your web browser. You will end up with a spider-web style diagram. Quite often you will have to click the `back` button, to get back to the page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb92215",
   "metadata": {},
   "source": [
    "Here we have questions:\n",
    "- Can you reach all the html files in the folder?\n",
    "- Are some pages dead ends? (you can't click to anywhere from them)\n",
    "- What page is most pointed to? And what page points to the most other pages?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dd5005",
   "metadata": {},
   "source": [
    "## Let's try to automate the above process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c566ea94",
   "metadata": {},
   "source": [
    "We will\n",
    "- visit the first page and store all the links we found\n",
    "- visit all these links in turn. But only if we have not visited them already (to avoid an infinite loop).\n",
    "\n",
    "Therefore, we need a function that can visit a website and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b802eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visit_page_and_return_links(page_url):\n",
    "    page_url_in_a_folder  = create_local_file_address(\"demowebsite\", page_url)\n",
    "    # note, this assumes you have your html files in a folder called demowebsite (like they are on github)\n",
    "    print(f\"Looking for links in {page_url_in_a_folder}\")\n",
    "    \n",
    "    html_of_website = urlopen(page_url_in_a_folder)\n",
    "    soup = BeautifulSoup(html_of_website, 'html.parser')\n",
    "\n",
    "    # .find_all links\n",
    "    links = soup.find_all('a')\n",
    "    link_urls = []\n",
    "    \n",
    "    # remember, links look like this: <a href=\"team.html\">Team</a>\n",
    "    # but we only care about the content of href attribute (href == 'html reference')\n",
    "    for link in links:\n",
    "        print('complete html of link: ', link, \"\\t points to \", link['href'])\n",
    "        #  add every link you have to the output list       \n",
    "        link_urls.append(link['href'])\n",
    "    # return output list with all urls          \n",
    "    return link_urls\n",
    "        \n",
    "    \n",
    "starting_website = \"home.html\"\n",
    "    \n",
    "found_links = visit_page_and_return_links(starting_website)\n",
    "print(\"Found links:\", found_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8927f8b2",
   "metadata": {},
   "source": [
    "Now we can write a loop that will visit all the pages. And when it visits them, will keep track of where they point. This could be done with classes, dictionaries or numpys. We'll use a list of dictionaries, but feel free to translate it into numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0a062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we are done, each page info will look like this:\n",
    "demo_page = {\"address\":\"home.html\", \"links_to\": ['team.html', 'news.html', 'business_deals.html', 'shop.html'] }\n",
    "\n",
    "pp.pprint(demo_page) #note pp.pprint() is a 'pretty version of print'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79af364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visit our starting page, but first we'll make our function return above format of data\n",
    "\n",
    "def visit_page_and_return_dictionary(page_url):\n",
    "    #  this part is as before:   \n",
    "    page_url_in_a_folder  = \"file:///\" + os.getcwd() + \"/demowebsite/\" + page_url\n",
    "    print(f\"Looking for links in {page_url}\")\n",
    "    html_of_website = urlopen(page_url_in_a_folder)\n",
    "    soup = BeautifulSoup(html_of_website, 'html.parser')\n",
    "    links = soup.find_all('a')\n",
    "    link_urls = []\n",
    "    for link in links:    \n",
    "        link_urls.append(link['href'])\n",
    "        \n",
    "    # this is new: do not just return a list. rather, returned a structured page info     \n",
    "    return {'address': page_url, \n",
    "            'links_to': link_urls}\n",
    "    \n",
    "starting_website = \"home.html\"\n",
    "page_info = visit_page_and_return_dictionary(starting_website)\n",
    "print()\n",
    "pp.pprint(page_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe59130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will start two lists: Pages we have visited, and pages we have not visited yet:\n",
    "# note that there are countless ways to do that.\n",
    "\n",
    "starting_website = \"home.html\"\n",
    "\n",
    "pages_we_visited = []\n",
    "pages_to_visit = [starting_website] # this is new. LEt's add starting page to the pages_to_visit at first\n",
    "pages_scraped_info = []\n",
    "\n",
    "next_page_to_visit = pages_to_visit.pop() # we grab and remove the first page to visit\n",
    "page_info = visit_page_and_return_dictionary(next_page_to_visit)\n",
    "pages_scraped_info.append(page_info)\n",
    "\n",
    "print(\"page_info\", page_info)\n",
    "\n",
    "pages_we_visited.append(page_info['address']) # we visited this page\n",
    "\n",
    "# for all links, if they were not yet visited, add them to pages_to_visit\n",
    "for link_url in page_info['links_to']:\n",
    "    if link_url not in pages_we_visited:\n",
    "        pages_to_visit.append(link_url)\n",
    "        \n",
    "print()\n",
    "print(\"pages_we_visited\", pages_we_visited)\n",
    "print(\"pages_to_visit\", pages_to_visit)\n",
    "print(\"pages_scraped_info\", pages_scraped_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c6aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now all we need to do is to repeat this process while there are any pages left in pages_to_visit\n",
    "\n",
    "# this is the same\n",
    "starting_website = \"home.html\"\n",
    "pages_we_visited = []\n",
    "pages_to_visit = [starting_website] \n",
    "pages_scraped_info = []\n",
    "\n",
    "# keep repeating your code, until there are no more pages to visit\n",
    "while len(pages_to_visit) > 0:\n",
    "    # rest is the same\n",
    "    next_page_to_visit = pages_to_visit.pop() \n",
    "    page_info = visit_page_and_return_dictionary(next_page_to_visit)\n",
    "    pages_scraped_info.append(page_info)\n",
    "\n",
    "    pages_we_visited.append(page_info['address']) \n",
    "    for link_url in page_info['links_to']:\n",
    "        if link_url not in pages_we_visited:\n",
    "            pages_to_visit.append(link_url)\n",
    "        \n",
    "\n",
    "        \n",
    "print()\n",
    "print(\"pages_we_visited\", pages_we_visited)\n",
    "print(\"pages_to_visit\", pages_to_visit)\n",
    "print(\"pages_scraped_info\")\n",
    "pp.pprint(pages_scraped_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c22085c",
   "metadata": {},
   "source": [
    "### Is this consistent with your notes that you created while manually clicking on page links?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa79acf8",
   "metadata": {},
   "source": [
    "When there are just a few pages, this process can be done manually, but as soon as you want to with no errors, and at scale (whcih is basically, always)... it is better to use software to do it for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b21344",
   "metadata": {},
   "source": [
    "## Drawing a simple graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc54bb",
   "metadata": {},
   "source": [
    "We will talk about more graphs next week, but now I will show you a very quick one. We will use the data from our exercise above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d625bcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a graph\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# add some edges\n",
    "graph.add_edge('A','B')\n",
    "graph.add_edge('A','D')\n",
    "graph.add_edge('A','C')\n",
    "graph.add_edge('C','B')\n",
    "\n",
    "# calculate 'elastic' layout\n",
    "positions = nx.spring_layout(graph)\n",
    "\n",
    "# draw the graph (you migh see a pink warning the first time. just ignore it)\n",
    "nx.draw(graph, positions, with_labels=True)\n",
    "plt.show()\n",
    "\n",
    "# notice tiny arrows indicating where graphs come from and go to:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c7ae2",
   "metadata": {},
   "source": [
    "### Now let's use pages_scraped_info to create a graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3169fc6b",
   "metadata": {},
   "source": [
    "Basically, for each page, we will add edges (arrows) to all the pages the page has links to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43333524",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph = nx.DiGraph()\n",
    "\n",
    "# for every page we scraped\n",
    "for page in pages_scraped_info:\n",
    "    # take link origin (basically, that page)\n",
    "    link_origin = page['address']\n",
    "    all_links = page['links_to']\n",
    "    # and for all pages it had links to (basically, the destinations)\n",
    "    for link_destination in all_links:\n",
    "        # create graph adges pointing from origin to destination\n",
    "        graph.add_edge(link_origin, link_destination)\n",
    "\n",
    "positions = nx.spring_layout(graph)\n",
    "nx.draw(graph, positions, with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e087eae8",
   "metadata": {},
   "source": [
    "Te make the figure look nicer, please visit https://networkx.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2970049f",
   "metadata": {},
   "source": [
    "#### Are these arrows consistent with your notes that you created while manually clicking on page links?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187f625a",
   "metadata": {},
   "source": [
    "Can you see how it could be useful to map and represent someone's web page automatically like that? And analyse it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e5f55e",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4d02a0",
   "metadata": {},
   "source": [
    "We'll talk a bit more about metrics next week, but here is a simple measure of PageRank. We could calculate it by hand, but it would be rather time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e466bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageranks = nx.pagerank(graph)\n",
    "\n",
    "pp.pprint(pageranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f42af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = nx.spring_layout(graph)\n",
    "\n",
    "size = [pagerank * 5000 for pagerank in pageranks.values()]\n",
    "\n",
    "nx.draw(graph, positions, with_labels= True, node_size = size)\n",
    "plt.title(\"Size scaled to PageRank\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6774e8e8",
   "metadata": {},
   "source": [
    "## Think about it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b1a5d",
   "metadata": {},
   "source": [
    "- What can you say about this website, knowing all the links and their page ranks?\n",
    "- Is this a good outcome? Is that there you would like your users to be?\n",
    "- how would you change the links structure to e.g. increase amount of predicted traffic to business_deals.html ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f3642",
   "metadata": {},
   "source": [
    "# Mini Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629e5784",
   "metadata": {},
   "source": [
    "If you feel adventurous try to implement your suggestions from above - *edit the html files to add/remove links, and then re-run this notebook from the top*. Did you achieve the task you wanted to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1044e00b",
   "metadata": {},
   "source": [
    "**Note about editing htmls files in jupyter:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72a54b",
   "metadata": {},
   "source": [
    "If you'd like to view and edit html of these pages, you can go to your list of files in Jupyter. In there in the folder called demowebsite you will find a set of html pages: to view and edit them as html: tick the ckeckbox next to a file and select 'edit' button on top of the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbcc0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
